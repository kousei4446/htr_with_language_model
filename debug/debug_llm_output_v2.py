"""
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®CTCå‡ºåŠ›ã¨LLMå‡ºåŠ›ã‚’æ¯”è¼ƒã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (æ–°å®Ÿè£…å¯¾å¿œç‰ˆ)
- æ–°ã—ã„MobileViT+LLMå®Ÿè£…ã«å¯¾å¿œ
- æ—§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚‚èª­ã¿è¾¼ã¿å¯èƒ½
"""
import sys
import os

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆmodelsã¨utilsã‚’importã™ã‚‹ãŸã‚ï¼‰
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from omegaconf import OmegaConf
from models import HTRNet
from utils.htr_dataset import HTRDataset
import matplotlib.pyplot as plt
from datetime import datetime
import numpy as np

# results/debug_llm/æ—¥ä»˜-æ™‚åˆ»/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ï¼‰
timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
results_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'results', 'debug_llm', timestamp)
os.makedirs(results_dir, exist_ok=True)
print(f"ğŸ“ Created/verified results directory: {results_dir}")

# è¨­å®šãƒ­ãƒ¼ãƒ‰ï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ï¼‰
config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'config.yaml')
config = OmegaConf.load(config_path)

device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
print("\nğŸ“‚ Loading dataset...")

# config.data.pathã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰è§£æ±º
data_path = config.data.path
if not os.path.isabs(data_path):
    # config.yamlãŒã‚ã‚‹è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆï¼‰ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_path = os.path.join(project_root, data_path)

dataset = HTRDataset(
    data_path,
    'test',  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ç¢ºèª
    fixed_size=(config.preproc.image_height, config.preproc.image_width)
)

# å­¦ç¿’æ™‚ã®æ–‡å­—ã‚¯ãƒ©ã‚¹ã‚’èª­ã¿è¾¼ã¿ï¼ˆdata_path/classes.npyã‹ã‚‰ï¼‰
classes_path = os.path.join(data_path, 'classes.npy')
classes = np.load(classes_path, allow_pickle=True).tolist()
print(f"Character classes: {len(classes)} different characters (loaded from training)")


def decode_ctc(tokens, char_classes):
    """
    CTCãƒ‡ã‚³ãƒ¼ãƒ‰: é‡è¤‡å‰Šé™¤ã¨blanké™¤å»

    Args:
        tokens: (seq_len,) ã®ãƒˆãƒ¼ã‚¯ãƒ³IDé…åˆ—
        char_classes: æ–‡å­—ãƒªã‚¹ãƒˆ

    Returns:
        ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸæ–‡å­—åˆ—
    """
    result = []
    prev = -1
    for t in tokens:
        if t != prev and t != 0:  # 0ã¯blank
            if t - 1 < len(char_classes):
                result.append(char_classes[t - 1])
        prev = t
    return ''.join(result)


# ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆLLMæœ‰åŠ¹ï¼‰
print("\nğŸ”§ Creating model with LLM enabled...")
llm_source = config.train.get('llm_source', 'rnn')
print(f"   LLM source: {llm_source}")

net = HTRNet(config.arch, len(classes) + 1, use_llm=True, llm_source=llm_source)

# å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ï¼‰
model_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                          'saved_models', 'CRNN+LLM', '700.pt')
print(f"\nğŸ“¥ Loading checkpoint: {model_path}")

if not os.path.exists(model_path):
    print(f"âŒ Error: Checkpoint not found at {model_path}")
    print("   Please update the model_path in this script")
    sys.exit(1)

load_dict = torch.load(model_path, map_location='cpu')

# æ—§å®Ÿè£…ã®LLMé–¢é€£ã‚­ãƒ¼ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ–°å®Ÿè£…ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
filtered_dict = {}
llm_keys_filtered = 0

for k, v in load_dict.items():
    # æ—§å®Ÿè£…ã®ã‚­ãƒ¼ï¼ˆtop.connector.*, top.llm.*ï¼‰ã‚’é™¤å¤–
    if k.startswith('top.connector.') or k.startswith('top.llm.'):
        llm_keys_filtered += 1
        continue
    filtered_dict[k] = v

print(f"   Filtered {llm_keys_filtered} LLM-related keys from checkpoint")

# ãƒ¢ãƒ‡ãƒ«ã«ãƒ­ãƒ¼ãƒ‰ï¼ˆstrict=False: æ–°ã—ã„connector_rnnç­‰ã¯åˆæœŸåŒ–ã•ã‚Œã‚‹ï¼‰
missing_keys, unexpected_keys = net.load_state_dict(filtered_dict, strict=True)

print(f"âœ… Loaded checkpoint successfully")
print(f"   Loaded: {len(filtered_dict)} parameters")
if missing_keys:
    print(f"   Missing keys: {len(missing_keys)} (new LLM components, initialized randomly)")
if unexpected_keys:
    print(f"   Unexpected keys: {len(unexpected_keys)}")

net.to(device)
net.eval()

# Connectoré¸æŠ
print("\nğŸ”§ Selecting Connector based on llm_source...")
connector = None
connector_name = None

if hasattr(net.top, 'connector_rnn') and net.top.connector_rnn is not None:
    connector = net.top.connector_rnn
    connector_name = 'RNN'
    print(f"   Using Connector_RNN (512 â†’ 3072)")
elif hasattr(net.top, 'connector_mv1') and net.top.connector_mv1 is not None:
    connector = net.top.connector_mv1
    connector_name = 'MobileViT1'
    print(f"   Using Connector_MV1 (64 â†’ 3072)")
elif hasattr(net.top, 'connector_mv2') and net.top.connector_mv2 is not None:
    connector = net.top.connector_mv2
    connector_name = 'MobileViT2'
    print(f"   Using Connector_MV2 (128 â†’ 3072)")
else:
    print("âŒ Error: No connector available!")
    print("   Check use_llm and llm_source in config.yaml")
    sys.exit(1)

# LLMãƒã‚§ãƒƒã‚¯
if not hasattr(net.top, 'llm') or net.top.llm is None:
    print("âŒ Error: LLM not available!")
    print("   Set use_llm: true in config.yaml")
    sys.exit(1)

print(f"âœ… LLM available (shared across all paths)")

# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã§ç¢ºèª
print("\nğŸ” Testing CTC vs LLM output on sample images...\n")
print("="*80)

num_samples = 5
indices = [5, 37, 12, 4, 67]  # å›ºå®šã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

for i, idx in enumerate(indices):
    img, transcr = dataset[idx]
    img_display = img.clone()  # è¡¨ç¤ºç”¨ã«ä¿å­˜
    img = img.unsqueeze(0).to(device)  # (1, 1, 128, 1024)

    print(f"\n{'='*80}")
    print(f"Sample {i+1}/{num_samples} (Index: {idx})")
    print(f"{'='*80}")
    print(f"ğŸ“ Ground Truth: '{transcr}'")

    with torch.no_grad():
        # 1. ç‰¹å¾´é‡æŠ½å‡º
        if net.stn is not None:
            img_feat = net.stn(img)
        else:
            img_feat = img
        y = net.features(img_feat)  # (1, 256, 1, width)

        # 2. RNNå‡¦ç†
        y_seq = y.permute(2, 3, 0, 1)[0]  # (width, 1, 256)
        y1 = net.top.rec1(y_seq)[0]  # (width, 1, 512)

        if net.top.recN is not None:
            y_rnn = net.top.recN(y1)[0]  # (width, 1, 512)
        else:
            y_rnn = y1

        # === CTCå‡ºåŠ› ===
        y_ctc = net.top.fnl(y_rnn)  # (width, 1, nclasses)
        ctc_tokens = torch.argmax(y_ctc, dim=-1).squeeze().cpu().numpy()  # (width,)
        ctc_text = decode_ctc(ctc_tokens, classes)
        print(f"ğŸ”¤ CTC Prediction: '{ctc_text}'")

        # === LLMå‡ºåŠ› ===
        # Connectorå…¥åŠ›æº–å‚™ï¼ˆRNN layer1å‡ºåŠ›ã‚’ä½¿ç”¨ï¼‰
        prefix_input = y1.permute(1, 0, 2)  # (1, width, 512)
        inputs_embeds = connector(prefix_input)  # (1, num_tokens, 3072)

        seq_len = inputs_embeds.shape[1]
        print(f"ğŸ”§ Connector output tokens: {seq_len} (using {connector_name} path)")

        # Ground Truthæ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        llm_labels = net.top.llm.tokenizer(
            [transcr],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=seq_len
        )
        labels = llm_labels["input_ids"].to(device)  # (1, seq_len)

        # LLMé †ä¼æ’­
        output_llm = net.top.llm.model(
            inputs_embeds=inputs_embeds.half(),
            labels=labels
        )

        # LLMäºˆæ¸¬ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        logits = output_llm.logits  # (1, seq_len, vocab_size)
        preds = torch.argmax(logits, dim=-1)  # (1, seq_len)
        pred_tokens = preds[0].cpu().numpy().tolist()
        llm_text = net.top.llm.tokenizer.decode(pred_tokens, skip_special_tokens=True)
        print(f"ğŸ¤– LLM Prediction: '{llm_text}'")

    # å€‹åˆ¥ç”»åƒã¨ã—ã¦ä¿å­˜
    fig, ax = plt.subplots(1, 1, figsize=(15, 3))
    ax.imshow(img_display.squeeze(), cmap='gray')
    ax.set_title(
        f"Sample {i+1} (Index: {idx}) - Using {connector_name} Connector\n"
        f"GT:  '{transcr}'\n"
        f"CTC: '{ctc_text}'\n"
        f"LLM: '{llm_text}'",
        fontsize=11,
        loc='left'
    )
    ax.axis('off')

    plt.tight_layout()
    output_path = os.path.join(results_dir, f'sample_{i+1}.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"ğŸ’¾ Saved: {output_path}")

print(f"\n{'='*80}")
print("âœ… Done!")
print(f"ğŸ“Š Images saved to: {results_dir}")
print(f"{'='*80}")

print("\nğŸ’¡ Note:")
print("   - æ–°ã—ã„MobileViT+LLMå®Ÿè£…ã«å¯¾å¿œ")
print(f"   - Connector: {connector_name} path")
print("   - CTC: å¾“æ¥ã®CTCãƒ‡ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›")
print("   - LLM: Connectorã‚’é€šã—ã¦LLMã§ç”Ÿæˆã—ãŸå‡ºåŠ›")
print("   - ä¸¡è€…ã®ç²¾åº¦ã‚’æ¯”è¼ƒã§ãã¾ã™")

"""
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®CTCå‡ºåŠ›ã¨LLMå‡ºåŠ›ã‚’æ¯”è¼ƒã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import sys
import os

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆmodelsã¨utilsã‚’importã™ã‚‹ãŸã‚ï¼‰
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from omegaconf import OmegaConf
from models import HTRNet
from utils.htr_dataset import HTRDataset
import matplotlib.pyplot as plt
from datetime import datetime
import numpy as np

# results/debug_llm/æ—¥ä»˜-æ™‚åˆ»/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ï¼‰
timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
results_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'results', 'debug_llm', timestamp)
os.makedirs(results_dir, exist_ok=True)
print(f"ğŸ“ Created/verified results directory: {results_dir}")

# è¨­å®šãƒ­ãƒ¼ãƒ‰ï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ï¼‰
config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'config.yaml')
config = OmegaConf.load(config_path)

device = torch.device(config.device if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
print("\nğŸ“‚ Loading dataset...")

# config.data.pathã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰è§£æ±º
data_path = config.data.path
if not os.path.isabs(data_path):
    # config.yamlãŒã‚ã‚‹è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆï¼‰ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_path = os.path.join(project_root, data_path)

dataset = HTRDataset(
    data_path,
    'test',  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ç¢ºèª
    fixed_size=(config.preproc.image_height, config.preproc.image_width)
)

# å­¦ç¿’æ™‚ã®æ–‡å­—ã‚¯ãƒ©ã‚¹ã‚’èª­ã¿è¾¼ã¿ï¼ˆsaved_models/classes.npyã‹ã‚‰ï¼‰
classes_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                           'saved_models', 'classes.npy')
classes = np.load(classes_path, allow_pickle=True).tolist()
print(f"Character classes: {len(classes)} different characters (loaded from training)")


def decode_ctc(tokens, char_classes):
    """
    CTCãƒ‡ã‚³ãƒ¼ãƒ‰: é‡è¤‡å‰Šé™¤ã¨blanké™¤å»

    Args:
        tokens: (seq_len,) ã®ãƒˆãƒ¼ã‚¯ãƒ³IDé…åˆ—
        char_classes: æ–‡å­—ãƒªã‚¹ãƒˆ

    Returns:
        ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸæ–‡å­—åˆ—
    """
    result = []
    prev = -1
    for t in tokens:
        if t != prev and t != 0:  # 0ã¯blank
            if t - 1 < len(char_classes):
                result.append(char_classes[t - 1])
        prev = t
    return ''.join(result)


# ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆLLMæœ‰åŠ¹ï¼‰
print("\nğŸ”§ Creating model with LLM enabled...")
net = HTRNet(config.arch, len(classes) + 1, use_llm=True)

# å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ï¼‰
model_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                          'saved_models', '10-30_llmmobilevit', '700.pt')
print(f"\nğŸ“¥ Loading checkpoint: {model_path}")
load_dict = torch.load(model_path, map_location='cpu')
missing_keys, unexpected_keys = net.load_state_dict(load_dict, strict=True)

print(f"âœ… Loaded checkpoint successfully")
print(f"   Model: 10-30_llmmobilevit/700.pt (trained with LLM)")
if missing_keys:
    print(f"   Missing keys: {len(missing_keys)}")
if unexpected_keys:
    print(f"   Unexpected keys: {len(unexpected_keys)}")

net.to(device)
net.eval()

# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã§ç¢ºèª
print("\nğŸ” Testing CTC vs LLM output on sample images...\n")
print("="*80)

num_samples = 5
indices = [5, 37, 12, 4, 67]  # å›ºå®šã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

for i, idx in enumerate(indices):
    img, transcr = dataset[idx]
    img_display = img.clone()  # è¡¨ç¤ºç”¨ã«ä¿å­˜
    img = img.unsqueeze(0).to(device)  # (1, 1, 128, 1024)

    print(f"\n{'='*80}")
    print(f"Sample {i+1}/{num_samples} (Index: {idx})")
    print(f"{'='*80}")
    print(f"ğŸ“ Ground Truth: '{transcr}'")

    with torch.no_grad():
        # 1. ç‰¹å¾´é‡æŠ½å‡º
        if net.stn is not None:
            img_feat = net.stn(img)
        else:
            img_feat = img
        y = net.features(img_feat)  # (1, 256, 1, width)

        # 2. RNNå‡¦ç†
        y_seq = y.permute(2, 3, 0, 1)[0]  # (width, 1, 256)
        y1 = net.top.rec1(y_seq)[0]  # (width, 1, 512)
        y_rnn = net.top.recN(y1)[0]  # (width, 1, 512)

        # === CTCå‡ºåŠ› ===
        y_ctc = net.top.fnl(y_rnn)  # (width, 1, nclasses)
        ctc_tokens = torch.argmax(y_ctc, dim=-1).squeeze().cpu().numpy()  # (width,)
        ctc_text = decode_ctc(ctc_tokens, classes)
        print(f"ğŸ”¤ CTC Prediction: '{ctc_text}'")

        # === LLMå‡ºåŠ› ===
        # Connectorå…¥åŠ›æº–å‚™
        prefix_input = y1.permute(1, 0, 2)  # (1, width, 512)
        inputs_embeds = net.top.connector(prefix_input)  # (1, num_tokens, 3072)

        seq_len = inputs_embeds.shape[1]
        print(f"ğŸ”§ Connector output tokens: {seq_len}")

        # Ground Truthæ–‡å­—åˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        llm_labels = net.top.llm.tokenizer(
            [transcr],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=seq_len
        )
        labels = llm_labels["input_ids"].to(device)  # (1, seq_len)

        # LLMé †ä¼æ’­
        output_llm = net.top.llm.model(
            inputs_embeds=inputs_embeds.half(),
            labels=labels
        )

        # LLMäºˆæ¸¬ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        logits = output_llm.logits  # (1, seq_len, vocab_size)
        preds = torch.argmax(logits, dim=-1)  # (1, seq_len)
        pred_tokens = preds[0].cpu().numpy().tolist()
        llm_text = net.top.llm.tokenizer.decode(pred_tokens, skip_special_tokens=True)
        print(f"ğŸ¤– LLM Prediction: '{llm_text}'")

    # å€‹åˆ¥ç”»åƒã¨ã—ã¦ä¿å­˜
    fig, ax = plt.subplots(1, 1, figsize=(15, 3))
    ax.imshow(img_display.squeeze(), cmap='gray')
    ax.set_title(
        f"Sample {i+1} (Index: {idx})\n"
        f"GT:  '{transcr}'\n"
        f"CTC: '{ctc_text}'\n"
        f"LLM: '{llm_text}'",
        fontsize=11,
        loc='left'
    )
    ax.axis('off')

    plt.tight_layout()
    output_path = os.path.join(results_dir, f'sample_{i+1}.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"ğŸ’¾ Saved: {output_path}")

print(f"\n{'='*80}")
print("âœ… Done!")
print(f"ğŸ“Š Images saved to: {results_dir}")
print(f"{'='*80}")

print("\nğŸ’¡ Note:")
print("   - å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆ700 epoch, LLMè¾¼ã¿ï¼‰ã‚’ä½¿ç”¨")
print("   - CTC: å¾“æ¥ã®CTCãƒ‡ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›")
print("   - LLM: Connectorã‚’é€šã—ã¦LLMã§ç”Ÿæˆã—ãŸå‡ºåŠ›")
print("   - ä¸¡è€…ã®ç²¾åº¦ã‚’æ¯”è¼ƒã§ãã¾ã™")

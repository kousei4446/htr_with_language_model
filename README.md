# HTR with LLM Integration (MobileViT + Llama-3.2-3B)

æ‰‹æ›¸ãæ–‡å­—èªè­˜ï¼ˆHTRï¼‰ã«Large Language Modelï¼ˆLLMï¼‰ã‚’çµ±åˆã—ãŸæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè£…ã§ã™ã€‚MobileViTãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨Llama-3.2-3Bã‚’çµ„ã¿åˆã‚ã›ã€æ–‡è„ˆç†è§£ã¨æ›–æ˜§ãªæ–‡å­—ã®è£œæ­£èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€[Best Practices for a Handwritten Text Recognition System](https://arxiv.org/abs/2404.11339) (DAS 2022) ã®å…¬å¼å®Ÿè£…ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€LLMçµ±åˆæ©Ÿèƒ½ã‚’è¿½åŠ ã—ãŸã‚‚ã®ã§ã™ã€‚

---

## ä¸»ãªç‰¹å¾´

### ğŸš€ LLMçµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **Hybrid Backbone**: MobileViT + CRNNï¼ˆè»½é‡ã‹ã¤é«˜æ€§èƒ½ï¼‰
- **Dual-Head Design**: RNN Headï¼ˆä¸»å‡ºåŠ›ï¼‰ + CNN Shortcutï¼ˆè£œåŠ©å‡ºåŠ›ï¼‰
- **LLM Augmentation**: Llama-3.2-3Bï¼ˆ3Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å‡çµæ¸ˆã¿ï¼‰
- **Q-Former Connector**: 128ãƒˆãƒ¼ã‚¯ãƒ³â†’64ãƒˆãƒ¼ã‚¯ãƒ³ã«åœ§ç¸®ã—ã€3072æ¬¡å…ƒã«æ‹¡å¼µ

### âš¡ åŠ¹ç‡çš„ãªå­¦ç¿’æˆ¦ç•¥
- **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹å­¦ç¿’**: ãƒãƒƒãƒã®1/8ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã«LLMã‚’é©ç”¨
- **è¤‡åˆæå¤±é–¢æ•°**: CTCæå¤± + LLMå› æœè¨€èªãƒ¢ãƒ‡ãƒ«æå¤±
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡**: LLMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å‡çµã€Connectorã®ã¿å­¦ç¿’ï¼ˆç´„4.92M paramsï¼‰

### ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- IAMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®é«˜ç²¾åº¦ï¼ˆCER 4.2%ã‚’ç›®æ¨™ï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ~6GBï¼ˆLlama-3.2-3Bä½¿ç”¨æ™‚ï¼‰
- LLMç„¡åŠ¹åŒ–ãƒ¢ãƒ¼ãƒ‰ã§é«˜é€Ÿæ¨è«–ã‚‚å¯èƒ½

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```mermaid
flowchart TB
  subgraph INPUT["Input"]
    I[Image<br/>128Ã—1024]
  end

  subgraph BACKBONE["Backbone (Hybrid CRNN + MobileViT)"]
    B0[Conv 7x7, 32]
    B1[ResBlock x2 -> 64]
    MP1[MaxPool]
    MV1[MobileViT Block 1]
    B2[ResBlock x4 -> 128]
    MP2[MaxPool]
    MV2[MobileViT Block 2]
    B3[ResBlock x4 -> 256]
    COLMP[Column MaxPool<br/>-> Temporal Features]
  end

  subgraph HEADS["Heads"]
    subgraph RNN_HEAD["Recurrent Head"]
      R1[BiLSTM x3<br/>hidden=256]
      Rproj[LayerNorm + Linear + GELU]
      RCTC[Linear -> CTC logits]
    end
    subgraph CNN_SHORT["CTC shortcut"]
      Cconv[Conv -> CTC logits]
    end
  end

  subgraph LLM_PATH["LLM path (training only)"]
    INTER[Intermediate Feature<br/>e.g. RNN layer1 output]
    QFORMER[Connector: Q-Former<br/>128â†’64 tokens]
    EXPAND[Linear Expansions<br/>â†’ 3072-d embeddings]
    LLaMA[LLaMA-3.2-3B frozen<br/>causal LM loss]
    LAIL[LLM CLM Loss<br/>= LAIL]
  end

  I --> B0 --> B1 --> MP1 --> MV1 --> B2 --> MP2 --> MV2 --> B3 --> COLMP
  COLMP --> R1 --> Rproj --> RCTC
  COLMP --> Cconv

  %% LLM path connections
  %% (note: illustrate extraction point)
  R1 -->|sampled subset| INTER
  INTER --> QFORMER --> EXPAND --> LLaMA
  LLaMA --> LAIL
  RCTC -->|LCTC| Ltotal["Total Loss<br/>Ltotal = LCTC + Î± * LLAIL"]
  LAIL --> Ltotal

```

---

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### å¿…è¦ç’°å¢ƒ
- Python 3.9+
- PyTorch 2.0+ with CUDA 11.7+
- transformers (HuggingFace)
- 16GB+ GPU RAMï¼ˆLLMä½¿ç”¨æ™‚ï¼‰

### ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
conda create -n htr-llm python=3.9
conda activate htr-llm
pip install -r requirements.txt
```

**å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**:
```bash
pip install torch torchvision transformers accelerate
pip install albumentations nltk pyyaml
```

---

## ãƒ‡ãƒ¼ã‚¿æº–å‚™

### IAMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
1. [IAM Handwriting Database](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database) ã«ç™»éŒ²
2. ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:
   - `formsA-D.tgz`, `formsE-H.tgz`, `formsI-Z.tgz`ï¼ˆãƒ•ã‚©ãƒ¼ãƒ ç”»åƒï¼‰
   - `xml.tgz`ï¼ˆXMLã‚°ãƒ©ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ï¼‰

3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™:
```bash
python prepare_iam.py $mypath$/IAM/forms/ $mypath$/IAM/xml/ ./data/IAM/splits/ ./data/IAM/processed_lines
```

---

## è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆconfig.yamlï¼‰

### åŸºæœ¬è¨­å®š
```yaml
device: 'cuda:0'

data:
  path: './data/IAM/processed_lines'

arch:
  cnn_cfg: [[2, 64], 'M', "mobilevit1", [3, 128], 'M', "mobilevit2", [2, 256]]
  head_type: 'both'  # RNN + CNN shortcut
  rnn_type: 'lstm'
  rnn_layers: 3
  rnn_hidden_size: 256
```

### LLMçµ±åˆè¨­å®š
```yaml
train:
  use_llm: true              # LLMçµ±åˆã‚’æœ‰åŠ¹åŒ–
  llm_sample_ratio: 0.0625   # ãƒãƒƒãƒã®1/8ã«LLMã‚’é©ç”¨
  lr: 1e-3
  num_epochs: 800
  batch_size: 16
```

**LLMç„¡åŠ¹åŒ–ï¼ˆé«˜é€Ÿå­¦ç¿’ï¼‰**:
```yaml
train:
  use_llm: false  # LLMã‚’ä½¿ã‚ãªã„é€šå¸¸ã®HTRå­¦ç¿’
```

---

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

### LLMçµ±åˆãƒ¢ãƒ¼ãƒ‰ã§ã®å­¦ç¿’
```bash
python trainer.py config.yaml
```

**åˆå›å®Ÿè¡Œæ™‚**: Llama-3.2-3Bãƒ¢ãƒ‡ãƒ«ãŒè‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ï¼ˆç´„6GBï¼‰

### GPUã®æŒ‡å®š
```bash
CUDA_VISIBLE_DEVICES=0 python trainer.py config.yaml
```

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸Šæ›¸ã
```bash
python trainer.py config.yaml train.lr=1e-3 train.batch_size=32 train.use_llm=true
```

### LLMç„¡åŠ¹åŒ–ã§ã®é«˜é€Ÿå­¦ç¿’
```bash
python trainer.py config.yaml train.use_llm=false
```

---

## è©•ä¾¡

### ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
```bash
python evaluate.py config.yaml resume=./saved_models/htrnet.pt
```

### å˜ä¸€ç”»åƒã§ã®ãƒ‡ãƒ¢
```bash
python demo.py config.yaml resume=./saved_models/htrnet.pt ./data/IAM/processed_lines/test/c04-165-05.png
```

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è©³ç´°

### 1. Backbone (HybridBackboneCRNNMobileViT)
- **å…¥åŠ›**: 128Ã—1024ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒ
- **æ§‹æˆ**:
  - åˆæœŸç•³ã¿è¾¼ã¿ï¼ˆ7Ã—7, stride=[4,2]ï¼‰
  - ResNet-style BasicBlocks
  - 2ã¤ã®MobileViTãƒ–ãƒ­ãƒƒã‚¯ï¼ˆãƒ‘ãƒƒãƒã‚µã‚¤ã‚º4, 8ï¼‰
  - Column MaxPoolï¼ˆç¸¦æ–¹å‘ã‚’åœ§ç¸®ï¼‰

### 2. Dual-Head Design (CTCtopB)
#### RNN Headï¼ˆä¸»å‡ºåŠ›ï¼‰
- 3å±¤åŒæ–¹å‘LSTMï¼ˆhidden=256ï¼‰
- LayerNorm + Linear + GELUï¼ˆä¸­é–“ç‰¹å¾´é‡ã®çµ±åˆï¼‰
- CTCæå¤±ã§å­¦ç¿’

#### CNN Shortcutï¼ˆè£œåŠ©å‡ºåŠ›ï¼‰
- 1Ã—3ç•³ã¿è¾¼ã¿
- å‹¾é…ãƒ•ãƒ­ãƒ¼ã®æ”¹å–„

### 3. LLM Integrationï¼ˆå­¦ç¿’æ™‚ã®ã¿ï¼‰
#### Q-Former Connector
- **å½¹å‰²**: RNNç‰¹å¾´é‡ã‚’LLMå…¥åŠ›ã«å¤‰æ›
- **å‡¦ç†ãƒ•ãƒ­ãƒ¼**:
  1. RNNç¬¬1å±¤å‡ºåŠ›ï¼ˆ512æ¬¡å…ƒï¼‰ã‚’å–å¾—
  2. Q-Formerã§128â†’64ãƒˆãƒ¼ã‚¯ãƒ³ã«åœ§ç¸®
  3. Linearå±¤ã§512â†’1024â†’3072æ¬¡å…ƒã«æ‹¡å¼µ
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: ç´„6.06M

#### LLM (Llama-3.2-3B)
- **ãƒ¢ãƒ‡ãƒ«**: meta-llama/Llama-3.2-3B
- **çŠ¶æ…‹**: å®Œå…¨å‡çµï¼ˆå­¦ç¿’å¯¾è±¡å¤–ï¼‰
- **æå¤±**: å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°æå¤±ï¼ˆCLMï¼‰
- **é©ç”¨**: ãƒãƒƒãƒã®1/8ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿

### 4. æå¤±é–¢æ•°
```
L_total = L_CTC + Î± * L_LLM
```
- **L_CTC**: RNN Headã¨CNN Shortcutã®å‡ºåŠ›ã«é©ç”¨
- **L_LLM**: LLMã®å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°æå¤±ï¼ˆé¸æŠã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰
- **Î±**: LLMæå¤±ã®é‡ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰

---

## ãƒ¢ãƒ‡ãƒ«ã®åˆ‡ã‚Šæ›¿ãˆ

### ã‚ˆã‚Šè»½é‡ãªLLMã«å¤‰æ›´
`models.py:264`ã‚’ç·¨é›†:
```python
# ç¾åœ¨ï¼ˆ3Bã€ãƒ¡ãƒ¢ãƒª6GBï¼‰
model_name: str = "meta-llama/Llama-3.2-3B"

# ã•ã‚‰ã«è»½é‡åŒ–ï¼ˆ1Bã€ãƒ¡ãƒ¢ãƒª2GBï¼‰
model_name: str = "meta-llama/Llama-3.2-1B"
```

**æ³¨æ„**: ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã—ãŸå ´åˆã€`hidden_size`ã«å¿œã˜ã¦`models.py:246`ã®Connectorå‡ºåŠ›æ¬¡å…ƒã‚‚èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ¯”è¼ƒ

| ãƒ¢ãƒ¼ãƒ‰ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | å­¦ç¿’é€Ÿåº¦ | ç²¾åº¦ï¼ˆæœŸå¾…å€¤ï¼‰ |
|--------|-------------|---------|---------------|
| LLMçµ±åˆï¼ˆ3Bï¼‰ | ~6GB | ä¸­é€Ÿ | æœ€é«˜ |
| LLMçµ±åˆï¼ˆ1Bï¼‰ | ~2GB | é«˜é€Ÿ | é«˜ |
| LLMç„¡åŠ¹åŒ– | ~1GB | æœ€é«˜é€Ÿ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ |

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### CUDA Out of Memory
```yaml
# batch_sizeã‚’æ¸›ã‚‰ã™
train:
  batch_size: 8

# ã¾ãŸã¯LLMé©ç”¨ç‡ã‚’ä¸‹ã’ã‚‹
train:
  llm_sample_ratio: 0.03125  # 1/16ã«å¤‰æ›´
```

### LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼
```bash
# HuggingFace CLIã§æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
huggingface-cli login
huggingface-cli download meta-llama/Llama-3.2-3B
```

---

## Citation

ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„:

### å…ƒã®è«–æ–‡ï¼ˆBest Practices for HTRï¼‰
```bibtex
@inproceedings{retsinas2022best,
  title={Best practices for a handwritten text recognition system},
  author={Retsinas, George and Sfikas, Giorgos and Gatos, Basilis and Nikou, Christophoros},
  booktitle={International Workshop on Document Analysis Systems},
  pages={247--259},
  year={2022},
  organization={Springer}
}
```

### ã“ã®LLMçµ±åˆå®Ÿè£…
```bibtex
@misc{htr-llm-integration,
  title={Handwritten Text Recognition with LLM Integration: MobileViT and Llama-3.2},
  author={Your Name},
  year={2024},
  note={Extended implementation based on Retsinas et al. (2022) with LLM augmentation}
}
```

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å…ƒã®[HTR-best-practices](https://github.com/georgeretsi/HTR-best-practices)ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ãŠã‚Šã€åŒã˜ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚

---

## ä»Šå¾Œã®æ‹¡å¼µäºˆå®š

- [ ] ã‚ˆã‚Šå¤šæ§˜ãªLLMãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ã‚µãƒãƒ¼ãƒˆï¼ˆGPT-2, Phi-3, Qwen2ãªã©ï¼‰
- [ ] LoRA/QLoRAã‚’ç”¨ã„ãŸLLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- [ ] ä»–ã®æ‰‹æ›¸ããƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆRIMES, CVLï¼‰ã¸ã®å¯¾å¿œ
- [ ] ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ï¼ˆç”»åƒ+éŸ³å£°ï¼‰ã®çµ±åˆ

---

## è¬è¾

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ä»¥ä¸‹ã®ç´ æ™´ã‚‰ã—ã„ç ”ç©¶ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«åŸºã¥ã„ã¦ã„ã¾ã™:
- [HTR-best-practices](https://github.com/georgeretsi/HTR-best-practices) by Retsinas et al.
- [Llama 3.2](https://huggingface.co/meta-llama) by Meta AI
- [MobileViT](https://arxiv.org/abs/2110.02178) by Apple
- [BLIP-2 Q-Former](https://arxiv.org/abs/2301.12597) by Salesforce Research
